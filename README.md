# Projeto Apache Airflow - Qualidade e An√°lise de Dados

Este projeto demonstra como utilizar o Apache Airflow com Python para criar pipelines de dados que realizam verifica√ß√µes de qualidade e exploram dados de um endpoint p√∫blico da cidade de Nova York.

O projeto √© composto por duas DAGs (Directed Acyclic Graphs) que exemplificam fluxos de trabalho distintos:

- **Uma DAG que realiza uma valida√ß√£o b√°sica de quantidade de registros.**
- **Uma DAG que executa uma an√°lise mais aprofundada dos dados para verificar sua consist√™ncia e integridade.**

---

## üß© Explica√ß√£o do C√≥digo

O arquivo da DAG (`dag_qualidade_e_analise.py`) cont√©m duas DAGs independentes:

### 1. `primeira_dag_valida_registros`

Esta DAG atua como um "port√£o de qualidade". Seu objetivo √© validar se o volume de dados extra√≠do da fonte atende a uma expectativa m√≠nima.

- **`captura_conta_dados`**: Utiliza `requests` e `pandas` para se conectar √† URL, carregar o JSON em um DataFrame e retornar a quantidade de registros.
- **`e_valida` (BranchOperator)**: Puxa o valor da contagem (via XCom) e, com base em uma condi√ß√£o (> 1000), decide qual caminho a DAG deve seguir.
- **`valido` / `nvalido`**: Tarefas simples que representam os fluxos de sucesso ou falha, respectivamente. Em um cen√°rio real, o caminho `nvalido` poderia acionar um alerta por e-mail ou uma notifica√ß√£o.

---

### 2. `dag_exploracao_dados`

Esta DAG executa uma an√°lise explorat√≥ria dos dados para garantir sua qualidade. As tarefas s√£o executadas em paralelo para otimizar o tempo de processamento.

- **`get_data_as_dataframe`**: A tarefa inicial que extrai os dados brutos da URL e os carrega em um DataFrame.
- **`check_data_quality`**: Verifica a integridade dos dados, imprimindo informa√ß√µes como a contagem de valores nulos e os tipos de dados de cada coluna.
- **`check_for_duplicates`**: Analisa o DataFrame para identificar e reportar a presen√ßa de linhas duplicadas.
- **`analyze_categorical_data`**: Fornece insights sobre a distribui√ß√£o de valores em colunas categ√≥ricas, como `meal_type` e `city`.

---

## üõ†Ô∏è Como Executar o C√≥digo

Siga estes passos para configurar e executar o projeto em seu ambiente local.

### 1. Pr√©-requisitos

Certifique-se de que voc√™ tem o **Python 3.8+** instalado.

---

### 2. Configura√ß√£o do Ambiente Python

Crie um ambiente virtual e ative-o para isolar as depend√™ncias do projeto:

```bash
python -m venv venv
source venv/bin/activate  # No Windows, use `venv\Scripts\activate`
```

### 3. Instala√ß√£o do Apache Airflow e Depend√™ncias

Instale o Airflow e as bibliotecas necess√°rias para o c√≥digo (pandas, requests).

```bash
pip install "apache-airflow[celery,postgres,google,s3]==2.7.2" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.7.2/constraints-3.8.txt"
pip install pandas requests
```

### 4. Inicializa√ß√£o do Airflow

Inicialize o banco de dados e crie um usu√°rio para acessar a interface web.

```bash
airflow db migrate
airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com
```

### 5. Configura√ß√£o da DAG

Copie o c√≥digo completo deste projeto e cole-o em um novo arquivo dentro da pasta de DAGs do seu Airflow (geralmente ~/airflow/dags/).

```bash
# Navegue at√© a pasta de DAGs do seu ambiente Airflow
cd ~/airflow/dags

# Crie o arquivo e cole o c√≥digo
nano dag_qualidade_e_analise.py
```

### 6. Execu√ß√£o

Inicie o scheduler e o servidor web do Airflow em terminais separados.

```bash
# Terminal 1: Iniciar o scheduler
airflow scheduler

# Terminal 2: Iniciar o servidor web
airflow webserver
```

Ap√≥s iniciar os servi√ßos, acesse a interface web em http://localhost:8080, encontre as duas DAGs e ative-as.